import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import Precision, Recall
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix


def create_advanced_risk_assessment_nn(input_shape, num_classes=4):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""

    # 1. –û—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Ç—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    def create_model_1():
        model = keras.Sequential(
            [
                layers.Dense(
                    128,
                    activation="relu",
                    input_shape=(input_shape,),
                    kernel_regularizer=regularizers.l2(0.001),
                ),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(
                    64, activation="relu", kernel_regularizer=regularizers.l2(0.001)
                ),
                layers.BatchNormalization(),
                layers.Dropout(0.2),
                layers.Dense(32, activation="relu"),
                layers.Dense(16, activation="relu"),
                layers.Dense(num_classes, activation="softmax"),
            ]
        )

        return model

    # 2. –°–µ—Ç—å —Å residual connections
    def create_model_2():
        inputs = layers.Input(shape=(input_shape,))

        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
        x = layers.Dense(256, activation="relu")(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.4)(x)

        # Residual connection
        residual = layers.Dense(128, activation="relu")(x)
        x = layers.Dense(128, activation="relu")(residual)
        x = layers.Add()([residual, x])
        x = layers.BatchNormalization()(x)

        x = layers.Dense(64, activation="relu")(x)
        x = layers.Dropout(0.3)(x)

        x = layers.Dense(32, activation="relu")(x)

        outputs = layers.Dense(num_classes, activation="softmax")(x)

        model = keras.Model(inputs=inputs, outputs=outputs)
        return model

    # 3. –®–∏—Ä–æ–∫–∞—è —Å–µ—Ç—å
    def create_model_3():
        model = keras.Sequential(
            [
                layers.Dense(256, activation="relu", input_shape=(input_shape,)),
                layers.BatchNormalization(),
                layers.Dropout(0.4),
                layers.Dense(128, activation="relu"),
                layers.Dropout(0.3),
                layers.Dense(64, activation="relu"),
                layers.Dense(32, activation="relu"),
                layers.Dense(num_classes, activation="softmax"),
            ]
        )

        return model

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
    models = [create_model_1(), create_model_2(), create_model_3()]

    return models


def prepare_features_for_nn(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å —É—á–µ—Ç–æ–º beta –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞"""

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ)
    basic_features = [
        "dividend_yield",  # –î–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        "P_E",  # P/E
        "P_B",  # P/B
        "P_S",  # P/S
        "NPM",  # –ß–∏—Å—Ç–∞—è –º–∞—Ä–∂–∞ –ø—Ä–∏–±—ã–ª–∏
        "EV_EBITDA",  # EV/EBITDA
        "ROE",  # –†–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–ø–∏—Ç–∞–ª–∞
        "debt_capital",  # –î–æ–ª–≥/–ö–∞–ø–∏—Ç–∞–ª
        "EPS",  # –ü—Ä–∏–±—ã–ª—å –Ω–∞ –∞–∫—Ü–∏—é
    ]

    # –†—ã–Ω–æ—á–Ω—ã–µ —Ä–∏—Å–∫–∏
    risk_features = [
        "–ë–µ—Ç–∞",  # Beta –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        "P_FCF",  # –¶–µ–Ω–∞/–°–≤–æ–±–æ–¥–Ω—ã–π –¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫
        "ROA",  # –†–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–æ–≤
        "ROIC",  # –†–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π
        "Debt_EBITDA",  # –î–æ–ª–≥/EBITDA
    ]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
    available_basic = [col for col in basic_features if col in df.columns]
    available_risk = [col for col in risk_features if col in df.columns]

    # –ï—Å–ª–∏ beta –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ–¥ –¥—Ä—É–≥–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    beta_aliases = ["–ë–µ—Ç–∞", "Beta", "beta", "–ë–µ—Ç–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç", "–ë–µ—Ç—Ç–∞", "–ë–ï–¢–ê"]
    beta_col = None
    for alias in beta_aliases:
        if alias in df.columns:
            beta_col = alias
            break

    all_features = available_basic + available_risk
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(all_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
    for i, feat in enumerate(all_features):
        print(f"  {i+1}. {feat}")

    if beta_col:
        print(f"‚úì Beta –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ —Å—Ç–æ–ª–±—Ü–µ: '{beta_col}'")
    else:
        print("‚úó Beta –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")

    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X = []
    tickers = []
    valid_indices = []

    for idx, row in df.iterrows():
        feature_vector = []
        valid = True

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for col in available_basic:
            val = row.get(col, None)
            if pd.isna(val):
                valid = False
                break
            try:
                feature_vector.append(float(val))
            except (ValueError, TypeError):
                valid = False
                break

        # –ï—Å–ª–∏ –Ω–µ—Ç –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not valid or len(feature_vector) < len(available_basic):
            continue

        # –†—ã–Ω–æ—á–Ω—ã–µ —Ä–∏—Å–∫–∏ (–¥–æ–±–∞–≤–ª—è–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN)
        for col in available_risk:
            val = row.get(col, None)
            if pd.isna(val):
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —Å—Ç–æ–ª–±—Ü—É
                col_median = (
                    df[col].median() if col in df.columns and not df[col].empty else 0
                )
                feature_vector.append(col_median)
            else:
                try:
                    feature_vector.append(float(val))
                except (ValueError, TypeError):
                    # –ï—Å–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω—É
                    col_median = (
                        df[col].median()
                        if col in df.columns and not df[col].empty
                        else 0
                    )
                    feature_vector.append(col_median)

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ beta
        if beta_col and beta_col in row:
            beta_val = row[beta_col]
            if not pd.isna(beta_val):
                try:
                    beta_float = float(beta_val)
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º beta –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–æ—Ç 0 –¥–æ 1)
                    beta_normalized = max(min(beta_float, 3.0), -1.0) / 3.0
                    feature_vector.append(beta_normalized)
                except (ValueError, TypeError):
                    feature_vector.append(0.5)  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            else:
                feature_vector.append(0.5)  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        else:
            feature_vector.append(0.5)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(feature_vector) == (len(available_basic) + len(available_risk) + 1):
            X.append(feature_vector)
            tickers.append(row.get("–¢–∏–∫–µ—Ä", f"Row_{idx}"))
            valid_indices.append(idx)

    if not X:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
        return None, None, None, None

    X = np.array(X)
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –∞–∫—Ü–∏–π —Å {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")

    return X, tickers, valid_indices, all_features


def calculate_risk_categories(df, use_ae_scores=True):
    """–†–∞—Å—Å—á–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ä–∏—Å–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""

    y = []
    category_details = []

    for idx, row in df.iterrows():
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ä–∏—Å–∫–∞
        risk_factors = []

        # 1. P/E —Ä–∏—Å–∫ (—á–µ–º –≤—ã—à–µ P/E, —Ç–µ–º –≤—ã—à–µ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏)
        p_e = row.get("P_E", 20)
        try:
            p_e_float = float(p_e)
            if p_e_float <= 0:
                p_e_risk = 3  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π P/E = –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
            elif p_e_float < 10:
                p_e_risk = 0  # –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫
            elif p_e_float < 20:
                p_e_risk = 1  # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
            elif p_e_float < 30:
                p_e_risk = 2  # –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
            else:
                p_e_risk = 3  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
        except (ValueError, TypeError):
            p_e_risk = 2  # –°—Ä–µ–¥–Ω–µ-–≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        risk_factors.append(p_e_risk)

        # 2. Debt/Capital —Ä–∏—Å–∫
        debt_cap = row.get("debt_capital", 0.5)
        try:
            debt_cap_float = float(debt_cap)
            if debt_cap_float < 0.3:
                debt_risk = 0
            elif debt_cap_float < 0.5:
                debt_risk = 1
            elif debt_cap_float < 0.7:
                debt_risk = 2
            else:
                debt_risk = 3
        except (ValueError, TypeError):
            debt_risk = 2

        risk_factors.append(debt_risk)

        # 3. ROE —Ä–∏—Å–∫ (—á–µ–º –Ω–∏–∂–µ ROE, —Ç–µ–º –≤—ã—à–µ —Ä–∏—Å–∫)
        roe = row.get("ROE", 0.1)
        try:
            roe_float = float(roe)
            if roe_float > 0.15:
                roe_risk = 0
            elif roe_float > 0.10:
                roe_risk = 1
            elif roe_float > 0.05:
                roe_risk = 2
            else:
                roe_risk = 3
        except (ValueError, TypeError):
            roe_risk = 2

        risk_factors.append(roe_risk)

        # 4. Beta —Ä–∏—Å–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        beta = row.get("–ë–µ—Ç–∞", 1.0)
        try:
            beta_float = float(beta)
            if beta_float < 0.7:
                beta_risk = 0
            elif beta_float < 1.0:
                beta_risk = 1
            elif beta_float < 1.3:
                beta_risk = 2
            else:
                beta_risk = 3
        except (ValueError, TypeError):
            beta_risk = 1  # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        risk_factors.append(beta_risk)

        # 5. –î–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∞—è = –≤—ã—à–µ —Ä–∏—Å–∫)
        div_yield = row.get("dividend_yield", row.get("–î–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", 0))
        try:
            if isinstance(div_yield, (int, float)):
                div_yield_float = div_yield
            else:
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É
                div_yield_float = float(div_yield) / 100  # –ï—Å–ª–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

            if div_yield_float > 0.08:
                div_risk = 0
            elif div_yield_float > 0.05:
                div_risk = 1
            elif div_yield_float > 0.02:
                div_risk = 2
            else:
                div_risk = 3
        except (ValueError, TypeError):
            div_risk = 2

        risk_factors.append(div_risk)

        # 6. –ê–Ω–æ–º–∞–ª–∏–∏ –æ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if use_ae_scores and "AE_–ê–Ω–æ–º–∞–ª–∏—è" in row:
            try:
                ae_anomaly = bool(row["AE_–ê–Ω–æ–º–∞–ª–∏—è"])
                ae_strong = row.get("AE_–°–∏–ª—å–Ω–∞—è_–∞–Ω–æ–º–∞–ª–∏—è", False)

                if ae_strong:
                    ae_risk = 3
                elif ae_anomaly:
                    ae_risk = 2
                else:
                    ae_risk = 0
            except:
                ae_risk = 0
        else:
            ae_risk = 0

        risk_factors.append(ae_risk)

        # –£—Å—Ä–µ–¥–Ω—è–µ–º —Ä–∏—Å–∫–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω—É–ª–∏ –æ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤)
        valid_risks = [r for r in risk_factors if r is not None]
        if valid_risks:
            avg_risk = np.mean(valid_risks)
        else:
            avg_risk = 2  # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if avg_risk < 1.0:
            category = 0  # 'A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫'
        elif avg_risk < 1.8:
            category = 1  # 'B: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫'
        elif avg_risk < 2.5:
            category = 2  # 'C: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫'
        else:
            category = 3  # 'D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫'

        y.append(category)
        category_details.append(
            {
                "avg_risk": avg_risk,
                "p_e_risk": p_e_risk,
                "debt_risk": debt_risk,
                "roe_risk": roe_risk,
                "beta_risk": beta_risk,
                "div_risk": div_risk,
                "ae_risk": ae_risk,
            }
        )

    return np.array(y), category_details


def train_risk_assessment_ensemble(df, n_folds=3, use_ae_results=True):
    """–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""

    print("=" * 80)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ù–ï–ô–†–û–°–ï–¢–ò –î–õ–Ø –û–¶–ï–ù–ö–ò –†–ò–°–ö–ê")
    print("=" * 80)

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X, tickers, valid_indices, feature_names = prepare_features_for_nn(df)

    if X is None:
        print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö!")
        return df, None, None

    # 2. –†–∞—Å—Å—á–µ—Ç —Ü–µ–ª–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    y, risk_details = calculate_risk_categories(df, use_ae_results)

    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∏–Ω–¥–µ–∫—Å—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏
    y_filtered = y[valid_indices]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    unique, counts = np.unique(y_filtered, return_counts=True)
    print("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ä–∏—Å–∫–∞:")

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    actual_categories = {}
    for cat, count in zip(unique, counts):
        if cat == 0:
            cat_name = "A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"
        elif cat == 1:
            cat_name = "B: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫"
        elif cat == 2:
            cat_name = "C: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫"
        elif cat == 3:
            cat_name = "D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫"
        else:
            cat_name = f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat}"

        actual_categories[cat] = cat_name
        print(f"  {cat_name}: {count} –∞–∫—Ü–∏–π ({count/len(y_filtered)*100:.1f}%)")

    # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 4. One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–π
    num_classes = len(actual_categories)
    print(f"\nüéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {num_classes}")

    if num_classes < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏! –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –∫–ª–∞—Å—Å–∞.")
        return df, None, None

    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ (—á—Ç–æ–±—ã –æ–Ω–∏ —à–ª–∏ –ø–æ–¥—Ä—è–¥ –æ—Ç 0)
    class_mapping = {
        old: new for new, old in enumerate(sorted(actual_categories.keys()))
    }
    y_mapped = np.array([class_mapping[cat] for cat in y_filtered])

    # One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    y_categorical = to_categorical(y_mapped, num_classes=num_classes)

    # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    actual_category_names = [
        actual_categories[old] for old in sorted(actual_categories.keys())
    ]

    # 5. –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

    # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã—Ö–æ–¥–æ–≤)
    models = create_advanced_risk_assessment_nn(X.shape[1], num_classes=num_classes)

    # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    ensemble_predictions = np.zeros_like(y_categorical)
    fold_metrics = []

    print("\n" + "=" * 80)
    print(f"–ù–ê–ß–ò–ù–ê–ï–ú –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Æ ({n_folds} —Ñ–æ–ª–¥–æ–≤)")
    print("=" * 80)

    for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y_mapped)):
        print(f"\nüîπ –û–±—É—á–µ–Ω–∏–µ fold {fold + 1}/{n_folds}")
        print(f"   Train: {len(train_idx)} samples, Val: {len(val_idx)} samples")

        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
        y_train, y_val = y_categorical[train_idx], y_categorical[val_idx]

        fold_model_predictions = []
        fold_model_accuracies = []

        # –û–±—É—á–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –≤ –∞–Ω—Å–∞–º–±–ª–µ
        for i, model in enumerate(models):
            print(f"   üß† –ú–æ–¥–µ–ª—å {i+1}/{len(models)}...", end=" ")

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            model.compile(
                optimizer=keras.optimizers.Adam(learning_rate=0.001),
                loss="categorical_crossentropy",
                metrics=["accuracy", Precision(), Recall()],
            )

            # Callbacks
            callbacks = [
                EarlyStopping(
                    monitor="val_loss", patience=8, restore_best_weights=True, verbose=0
                ),
                ReduceLROnPlateau(
                    monitor="val_loss", factor=0.5, patience=5, min_lr=1e-6, verbose=0
                ),
            ]

            # –û–±—É—á–µ–Ω–∏–µ
            history = model.fit(
                X_train,
                y_train,
                validation_data=(X_val, y_val),
                epochs=30,
                batch_size=16,
                callbacks=callbacks,
                verbose=0,
            )

            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            val_loss, val_acc, val_precision, val_recall = model.evaluate(
                X_val, y_val, verbose=0
            )
            fold_model_accuracies.append(val_acc)

            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            val_pred = model.predict(X_val, verbose=0)
            fold_model_predictions.append(val_pred)

            print(f"Accuracy: {val_acc:.3f}")

        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
        avg_pred = np.mean(fold_model_predictions, axis=0)
        ensemble_predictions[val_idx] = avg_pred

        # –ú–µ—Ç—Ä–∏–∫–∏ —Ñ–æ–ª–¥–∞
        fold_accuracy = np.mean(fold_model_accuracies)
        fold_metrics.append(
            {
                "fold": fold + 1,
                "accuracy": fold_accuracy,
                "model_accuracies": fold_model_accuracies,
            }
        )

    # 6. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)

    # –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Ñ–æ–ª–¥–∞–º
    avg_accuracy = np.mean([fm["accuracy"] for fm in fold_metrics])
    print(f"\nüìà –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è: {avg_accuracy:.3f}")

    # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –º–æ–¥–µ–ª—è–º
    print("\nüß† –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:")
    for i, model_metrics in enumerate(
        zip(*[fm["model_accuracies"] for fm in fold_metrics])
    ):
        avg_model_acc = np.mean(model_metrics)
        print(f"  –ú–æ–¥–µ–ª—å {i+1}: {avg_model_acc:.3f}")

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
    y_true = np.argmax(y_categorical, axis=1)
    y_pred = np.argmax(ensemble_predictions, axis=1)

    print("\nüìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:")
    try:
        print(
            classification_report(
                y_true, y_pred, target_names=actual_category_names, digits=3
            )
        )
    except:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å classification report")

    # 7. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ DataFrame
    df["NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ä–∏—Å–∫–∞"] = np.nan
    df["NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] = np.nan
    df["NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"] = ""

    # –û–±—Ä–∞—Ç–Ω–æ–µ –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    reverse_class_mapping = {new: old for old, new in class_mapping.items()}
    category_map_text = {
        0: "A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫",
        1: "B: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫",
        2: "C: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫",
        3: "D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫",
    }

    # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    for i, idx in enumerate(valid_indices):
        if i < len(y_pred):  # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
            original_category = reverse_class_mapping.get(y_pred[i], y_pred[i])
            df.at[idx, "NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ä–∏—Å–∫–∞"] = original_category
            df.at[idx, "NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] = np.max(ensemble_predictions[i])

            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if original_category in category_map_text:
                df.at[idx, "NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"] = category_map_text[original_category]
            else:
                df.at[idx, "NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"] = f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {original_category}"

    # 8. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤
    print("\n" + "=" * 80)
    print("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –†–ò–°–ö–û–í –ü–û –ê–ö–¶–ò–Ø–ú")
    print("=" * 80)

    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    predicted_categories = df["NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"].value_counts()

    # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –æ—Ç—á–µ—Ç–∞
    all_categories = [
        "A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫",
        "B: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫",
        "C: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫",
        "D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫",
    ]

    total_valid = predicted_categories.sum() if not predicted_categories.empty else 0

    for category in all_categories:
        count = predicted_categories.get(category, 0)
        percentage = count / total_valid * 100 if total_valid > 0 else 0
        print(f"{category:<30}: {count:>3} –∞–∫—Ü–∏–π ({percentage:.1f}%)")

    # 9. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\n" + "=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
    print("=" * 80)

    confidence_scores = df["NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"].dropna()
    if len(confidence_scores) > 0:
        print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_scores.mean():.3f}")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_scores.median():.3f}")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_scores.min():.3f}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_scores.max():.3f}")

    # 10. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    print("\n" + "=" * 80)
    print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 80)

    # –ê–∫—Ü–∏–∏ —Å –Ω–∏–∑–∫–∏–º —Ä–∏—Å–∫–æ–º (–µ—Å–ª–∏ —Ç–∞–∫–∏–µ –µ—Å—Ç—å)
    low_risk_stocks = df[df["NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"] == "A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"]
    if not low_risk_stocks.empty:
        print(f"\nüèÜ –¢–û–ü-5 –ê–ö–¶–ò–ô –° –ù–ò–ó–ö–ò–ú –†–ò–°–ö–û–ú:")
        print("-" * 70)
        for _, row in low_risk_stocks.head(5).iterrows():
            ticker = row.get("–¢–∏–∫–µ—Ä", "N/A")
            name = str(row.get("–ù–∞–∑–≤–∞–Ω–∏–µ", "N/A"))[:25]
            p_e = row.get("P_E", "N/A")
            div_yield = row.get("dividend_yield", row.get("–î–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", 0))
            beta = row.get("–ë–µ—Ç–∞", "N/A")
            confidence = row.get("NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", 0)

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥
            p_e_str = f"{float(p_e):.1f}" if isinstance(p_e, (int, float)) else str(p_e)
            div_yield_pct = (
                float(div_yield) * 100 if isinstance(div_yield, (int, float)) else 0
            )
            beta_str = (
                f"{float(beta):.2f}" if isinstance(beta, (int, float)) else str(beta)
            )

            print(
                f"{ticker:<8} {name:<25} "
                f"P/E: {p_e_str:<6} "
                f"–î–î: {div_yield_pct:.1f}% "
                f"Beta: {beta_str:<6} "
                f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}"
            )

    # –ê–∫—Ü–∏–∏ —Å –≤—ã—Å–æ–∫–∏–º —Ä–∏—Å–∫–æ–º
    high_risk_stocks = df[df["NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç"] == "D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫"]
    if not high_risk_stocks.empty:
        print(f"\n‚ö†Ô∏è  –ê–ö–¶–ò–ò –° –û–ß–ï–ù–¨ –í–´–°–û–ö–ò–ú –†–ò–°–ö–û–ú (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!):")
        print("-" * 70)
        for _, row in high_risk_stocks.head(3).iterrows():
            ticker = row.get("–¢–∏–∫–µ—Ä", "N/A")
            name = str(row.get("–ù–∞–∑–≤–∞–Ω–∏–µ", "N/A"))[:25]
            p_e = row.get("P_E", "N/A")
            debt_cap = row.get("debt_capital", "N/A")
            confidence = row.get("NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", 0)

            print(
                f"{ticker:<8} {name:<25} "
                f"P/E: {p_e if isinstance(p_e, str) else f'{p_e:.1f}':<6} "
                f"–î–æ–ª–≥/–ö–∞–ø–∏—Ç–∞–ª: {debt_cap if isinstance(debt_cap, str) else f'{debt_cap:.2f}':<5} "
                f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}"
            )

    return df, models, scaler


def get_risk_recommendations(df):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""

    recommendations = []

    for _, row in df.iterrows():
        ticker = row.get("–¢–∏–∫–µ—Ä", "Unknown")
        risk_category = row.get("NN_–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—Ç–µ–∫—Å—Ç", "")
        confidence = row.get("NN_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", 0)

        if pd.isna(risk_category) or risk_category == "":
            continue

        # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if risk_category == "A: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫":
            recommendation = {
                "ticker": ticker,
                "risk_level": "–ù–∏–∑–∫–∏–π",
                "action": "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –¥–ª—è –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è",
                "allocation": "5-15%",
                "monitoring": "–ï–∂–µ–∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ",
                "confidence": confidence,
            }
        elif risk_category == "B: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫":
            recommendation = {
                "ticker": ticker,
                "risk_level": "–°—Ä–µ–¥–Ω–∏–π",
                "action": "–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è",
                "allocation": "3-8%",
                "monitoring": "–ï–∂–µ–º–µ—Å—è—á–Ω–æ",
                "confidence": confidence,
            }
        elif risk_category == "C: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫":
            recommendation = {
                "ticker": ticker,
                "risk_level": "–í—ã—Å–æ–∫–∏–π",
                "action": "–¢–æ–ª—å–∫–æ –¥–ª—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤",
                "allocation": "1-3%",
                "monitoring": "–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ",
                "confidence": confidence,
            }
        elif risk_category == "D: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫":
            recommendation = {
                "ticker": ticker,
                "risk_level": "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π",
                "action": "–°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è, –≤—ã—Å–æ–∫–∞—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å",
                "allocation": "0-1%",
                "monitoring": "–ï–∂–µ–¥–Ω–µ–≤–Ω–æ",
                "confidence": confidence,
            }
        else:
            continue

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        beta = row.get("–ë–µ—Ç–∞", 1.0)
        if isinstance(beta, (int, float)):
            if beta > 1.5:
                recommendation["note"] = f"–í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (Beta={beta:.2f})"
            elif beta < 0.5:
                recommendation["note"] = f"–ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ä—ã–Ω–∫–æ–º (Beta={beta:.2f})"

        recommendations.append(recommendation)

    return recommendations


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é TensorFlow
        print(f"TensorFlow version: {tf.__version__}")
        print(f"Keras version: {keras.__version__}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        input_file = f"{parent_dir}/../data/fundamentals_shares.xlsx"
        output_file = f"{parent_dir}/../data/risk_assessment_results.xlsx"
        print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_excel(input_file)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

        # –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        df_with_risk, models, scaler = train_risk_assessment_ensemble(df, n_folds=3)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = get_risk_recommendations(df_with_risk)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        df_with_risk.to_excel(output_file, index=False)

        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ risk_assessment_results.xlsx")
        print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(recommendations)} –∞–∫—Ü–∏–π")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
        import traceback

        traceback.print_exc()
