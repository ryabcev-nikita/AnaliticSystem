# ==================== –ö–õ–ê–°–° –ú–û–î–ï–õ–ò –î–ï–†–ï–í–ê –†–ï–®–ï–ù–ò–ô ====================
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree

from ...tree_solver_models.tree_solver_constants.tree_solver_constants import (
    FILE_CONSTANTS,
    FORMATTING,
    MODEL_CONSTANTS,
    TARGET_MAPPING,
)
from ...tree_solver_models.tree_solver_market.market_analyzer import MarketAnalyzer


class DecisionTreeModel:
    """
    –ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π:
    - P/E (–¶–µ–Ω–∞/–ü—Ä–∏–±—ã–ª—å)
    - P/BV (–¶–µ–Ω–∞/–ë–∞–ª–∞–Ω—Å–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å)
    - ROE (–†–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞)
    - g (–¢–µ–º–ø —Ä–æ—Å—Ç–∞, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –≤ DataLoader)
    """

    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_columns = ["P/E", "P/BV", "ROE", "g"]  # –¢–µ–º–ø —Ä–æ—Å—Ç–∞ –∏–∑ DataLoader

        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫
        self.column_mapping = {
            "P/B": "P/BV",  # –ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö P/B, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ P/BV
        }

    def _map_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        df = df.copy()

        # P/BV –º–æ–∂–µ—Ç –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É
        if "P/B" in df.columns and "P/BV" not in df.columns:
            df["P/BV"] = df["P/B"]
        elif "P/BV" in df.columns and "P/B" not in df.columns:
            pass  # –£–∂–µ –µ—Å—Ç—å –Ω—É–∂–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞
        elif "P/B" not in df.columns and "P/BV" not in df.columns:
            print("   ‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ P/B –∏–ª–∏ P/BV")

        return df

    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        df = df.copy()

        # –ú–∞–ø–ø–∏–Ω–≥ –∫–æ–ª–æ–Ω–æ–∫
        df = self._map_columns(df)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        available_features = []
        missing_features = []

        for feat in self.feature_columns:
            if feat in df.columns:
                available_features.append(feat)
            else:
                missing_features.append(feat)

        if missing_features:
            print(f"   ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(missing_features)}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            self.feature_columns = available_features

        if not available_features:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")

        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
        if "–ù–∞–∑–≤–∞–Ω–∏–µ" in df.columns:
            df["–°–µ–∫—Ç–æ—Ä"] = df["–ù–∞–∑–≤–∞–Ω–∏–µ"].apply(MarketAnalyzer.assign_sector)
            df["–°–µ–∫—Ç–æ—Ä_encoded"] = self.label_encoder.fit_transform(df["–°–µ–∫—Ç–æ—Ä"])

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        print(f"\nüìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏: {', '.join(self.feature_columns)}")
        for feat in self.feature_columns:
            if feat in df.columns:
                non_na = df[feat].notna().sum()
                print(
                    f"   ‚Ä¢ {feat}: {non_na} –Ω–µ–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({non_na/len(df)*100:.1f}%)"
                )

        return df

    def _calculate_peg(self, row) -> float:
        """–†–∞—Å—á–µ—Ç PEG ratio –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏"""
        try:
            pe = row.get("P/E", np.nan)
            g = row.get("g", np.nan)

            if pd.notna(pe) and pd.notna(g) and pe > 0 and g > 0:
                return pe / g
            return np.nan
        except:
            return np.nan

    def _assign_target(self, row) -> int:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ P/E, P/BV, ROE –∏ g

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è PEG ratio –∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        required_cols = ["P/E", "P/BV", "ROE", "g"]
        if not all(
            pd.notna(row.get(col))
            for col in required_cols
            if col in self.feature_columns
        ):
            return np.nan

        pe = row.get("P/E", np.nan)
        pbv = row.get("P/BV", np.nan)
        roe = row.get("ROE", np.nan)
        g = row.get("g", np.nan)

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if pe <= 0 or pbv <= 0 or roe <= 0:
            return np.nan

        # –†–∞—Å—á–µ—Ç PEG ratio
        peg = pe / g if g > 0 else float("inf")

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤

        # STRONG BUY: –ö–æ–º–ø–∞–Ω–∏–∏ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º
        if (
            (peg < 0.5 and roe > 20 and pbv < 1.5)
            or (pe < 8 and roe > 25 and g > 15)
            or (pbv < 1 and roe > 15 and g > 10)
        ):
            return TARGET_MAPPING.STRONG_UNDERVALUED

        # BUY: –•–æ—Ä–æ—à–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —Ä–∞–∑—É–º–Ω–æ–π —Ü–µ–Ω–µ
        elif (
            (peg < 1 and roe > 15)
            or (pe < 12 and roe > 20)
            or (pbv < 1.5 and roe > 18 and g > 8)
        ):
            return TARGET_MAPPING.UNDERVALUED

        # OVERVALUED: –î–æ—Ä–æ–≥–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–ª–∏ —Å –Ω–∏–∑–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º
        elif (
            (peg > 2 and roe < 15)
            or (pe > 25 and g < 10)
            or (pbv > 3 and roe < 12)
            or (g < 5 and pe > 20)
        ):
            return TARGET_MAPPING.OVERVALUED

        # FAIR VALUE: –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        else:
            return TARGET_MAPPING.FAIR_VALUE

    def _calculate_fundamental_score(self, row) -> float:
        """
        –†–∞—Å—á–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ (0-100) –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        score = 0
        weights = {"pe": 0.25, "pbv": 0.20, "roe": 0.30, "g": 0.25}

        # –û—Ü–µ–Ω–∫–∞ –ø–æ P/E (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
        if pd.notna(row.get("P/E")):
            pe = row["P/E"]
            if pe < 5:
                score += weights["pe"] * 100
            elif pe < 10:
                score += weights["pe"] * 80
            elif pe < 15:
                score += weights["pe"] * 60
            elif pe < 20:
                score += weights["pe"] * 40
            elif pe < 25:
                score += weights["pe"] * 20

        # –û—Ü–µ–Ω–∫–∞ –ø–æ P/BV (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
        if pd.notna(row.get("P/BV")):
            pbv = row["P/BV"]
            if pbv < 0.5:
                score += weights["pbv"] * 100
            elif pbv < 1:
                score += weights["pbv"] * 80
            elif pbv < 1.5:
                score += weights["pbv"] * 60
            elif pbv < 2:
                score += weights["pbv"] * 40
            elif pbv < 3:
                score += weights["pbv"] * 20

        # –û—Ü–µ–Ω–∫–∞ –ø–æ ROE (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        if pd.notna(row.get("ROE")):
            roe = row["ROE"]
            if roe > 30:
                score += weights["roe"] * 100
            elif roe > 20:
                score += weights["roe"] * 80
            elif roe > 15:
                score += weights["roe"] * 60
            elif roe > 10:
                score += weights["roe"] * 40
            elif roe > 5:
                score += weights["roe"] * 20

        # –û—Ü–µ–Ω–∫–∞ –ø–æ —Ç–µ–º–ø—É —Ä–æ—Å—Ç–∞ g (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        if pd.notna(row.get("g")):
            g = row["g"]
            if g > 25:
                score += weights["g"] * 100
            elif g > 20:
                score += weights["g"] * 80
            elif g > 15:
                score += weights["g"] * 60
            elif g > 10:
                score += weights["g"] * 40
            elif g > 5:
                score += weights["g"] * 20

        return score

    def train(
        self, df: pd.DataFrame, use_stratification: bool = True, verbose: bool = True
    ):
        """
        –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π

        Parameters:
        -----------
        df : pd.DataFrame
            –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ P/E, P/BV, ROE, g)
        use_stratification : bool
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ —Å–µ–∫—Ç–æ—Ä–∞–º
        verbose : bool
            –î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        if verbose:
            print("üå≥ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self.prepare_features(df)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è g
        if "g" not in df.columns:
            print(
                "   ‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'g'. –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞ –±–µ–∑ —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞."
            )
            self.feature_columns = [f for f in self.feature_columns if f != "g"]

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        df["–û—Ü–µ–Ω–∫–∞"] = df.apply(self._assign_target, axis=1)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π
        df_model = df[df["–û—Ü–µ–Ω–∫–∞"].notna()].copy()

        if len(df_model) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

        if verbose:
            print(f"\nüìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df_model)} –∫–æ–º–ø–∞–Ω–∏–π")
            print("   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
            target_dist = df_model["–û—Ü–µ–Ω–∫–∞"].map(TARGET_MAPPING.LABELS).value_counts()
            for label, count in target_dist.items():
                print(f"     ‚Ä¢ {label}: {count} ({count/len(df_model)*100:.1f}%)")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = df_model[self.feature_columns].copy()
        y = df_model["–û—Ü–µ–Ω–∫–∞"]

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        for col in X.columns:
            if X[col].isna().any():
                median_val = X[col].median()
                X[col] = X[col].fillna(median_val)
                if verbose:
                    print(
                        f"   ‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ {col}: –º–µ–¥–∏–∞–Ω–∞ = {median_val:.2f}"
                    )

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_scaled = self.scaler.fit_transform(X)

        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        stratify = (
            df_model["–°–µ–∫—Ç–æ—Ä_encoded"]
            if use_stratification and "–°–µ–∫—Ç–æ—Ä_encoded" in df_model.columns
            else y
        )

        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled,
            y,
            test_size=MODEL_CONSTANTS.TEST_SIZE,
            random_state=MODEL_CONSTANTS.RANDOM_STATE,
            stratify=stratify,
        )

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = DecisionTreeClassifier(
            max_depth=MODEL_CONSTANTS.MAX_DEPTH,
            min_samples_split=MODEL_CONSTANTS.MIN_SAMPLES_SPLIT,
            min_samples_leaf=MODEL_CONSTANTS.MIN_SAMPLES_LEAF,
            random_state=MODEL_CONSTANTS.RANDOM_STATE,
            class_weight="balanced",  # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            criterion="gini",  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'entropy' –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤—ã–∏–≥—Ä—ã—à–∞
        )

        self.model.fit(X_train, y_train)

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        train_pred = self.model.predict(X_train)
        test_pred = self.model.predict(X_test)

        results = {
            "train_accuracy": self.model.score(X_train, y_train),
            "test_accuracy": self.model.score(X_test, y_test),
            "feature_importance": dict(
                zip(self.feature_columns, self.model.feature_importances_)
            ),
            "class_distribution": df_model["–û—Ü–µ–Ω–∫–∞"].value_counts().to_dict(),
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "n_features": len(self.feature_columns),
            "n_classes": len(self.model.classes_),
            "tree_depth": self.model.get_depth(),
            "tree_leaves": self.model.get_n_leaves(),
        }

        # –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫
        from sklearn.metrics import confusion_matrix, classification_report

        results["train_confusion_matrix"] = confusion_matrix(y_train, train_pred)
        results["test_confusion_matrix"] = confusion_matrix(y_test, test_pred)
        results["classification_report"] = classification_report(
            y_test,
            test_pred,
            target_names=[
                TARGET_MAPPING.LABELS[i] for i in sorted(self.model.classes_)
            ],
            output_dict=True,
        )

        if verbose:
            print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ:")
            print(f"   ‚Ä¢ –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞: {results['tree_depth']}")
            print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤: {results['tree_leaves']}")
            print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {results['train_accuracy']:.2%}")
            print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {results['test_accuracy']:.2%}")

            print(f"\n   üìà –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for feat, imp in sorted(
                results["feature_importance"].items(), key=lambda x: x[1], reverse=True
            ):
                print(f"     ‚Ä¢ {feat}: {imp:.2%}")

        return results

    def predict(
        self, df: pd.DataFrame, add_fundamental_score: bool = True
    ) -> pd.DataFrame:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ü–∏–π

        Parameters:
        -----------
        df : pd.DataFrame
            –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        add_fundamental_score : bool
            –î–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ train().")

        df = df.copy()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self._map_columns(df)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        available_features = [f for f in self.feature_columns if f in df.columns]
        if not available_features:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∫—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if "–ù–∞–∑–≤–∞–Ω–∏–µ" in df.columns:
            df["–°–µ–∫—Ç–æ—Ä"] = df["–ù–∞–∑–≤–∞–Ω–∏–µ"].apply(MarketAnalyzer.assign_sector)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = df[available_features].copy()

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        for col in X.columns:
            if X[col].isna().any():
                X[col] = X[col].fillna(X[col].median())

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_scaled = self.scaler.transform(X)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        df["Predicted_–û—Ü–µ–Ω–∫–∞"] = self.model.predict(X_scaled)
        df["Predicted_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] = np.max(self.model.predict_proba(X_scaled), axis=1)
        df["Predicted_–û—Ü–µ–Ω–∫–∞_—Ç–µ–∫—Å—Ç"] = df["Predicted_–û—Ü–µ–Ω–∫–∞"].map(TARGET_MAPPING.LABELS)

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if "g" in df.columns and "P/E" in df.columns:
            df["PEG"] = np.where(
                (df["g"] > 0) & (df["P/E"] > 0), df["P/E"] / df["g"], np.nan
            )

        if add_fundamental_score:
            df["Fundamental_Score"] = df.apply(
                self._calculate_fundamental_score, axis=1
            )

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É
            def categorize_score(score):
                if pd.isna(score):
                    return "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
                elif score >= 80:
                    return "–û—Ç–ª–∏—á–Ω–æ"
                elif score >= 60:
                    return "–•–æ—Ä–æ—à–æ"
                elif score >= 40:
                    return "–°—Ä–µ–¥–Ω–µ"
                elif score >= 20:
                    return "–ù–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–≥–æ"
                else:
                    return "–°–ª–∞–±–æ"

            df["Fundamental_Category"] = df["Fundamental_Score"].apply(categorize_score)

        return df

    def plot_tree(
        self, filename: str = None, figsize: tuple = None, max_depth: int = 3
    ):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π

        Parameters:
        -----------
        filename : str
            –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        figsize : tuple
            –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
        max_depth : int
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ train().")

        if filename is None:
            filename = PATHS["decision_tree"]

        if figsize is None:
            figsize = FILE_CONSTANTS.FIGURE_SIZE_TREE

        plt.figure(figsize=figsize)

        # –ß–µ–ª–æ–≤–µ–∫–æ-–ø–æ–Ω—è—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_names_display = []
        for feat in self.feature_columns:
            if feat == "P/E":
                feature_names_display.append("P/E")
            elif feat == "P/BV":
                feature_names_display.append("P/BV")
            elif feat == "ROE":
                feature_names_display.append("ROE (%)")
            elif feat == "g":
                feature_names_display.append("–¢–µ–º–ø —Ä–æ—Å—Ç–∞ g (%)")
            else:
                feature_names_display.append(feat)

        class_names = [TARGET_MAPPING.LABELS[i] for i in sorted(self.model.classes_)]

        plot_tree(
            self.model,
            max_depth=max_depth,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
            feature_names=feature_names_display,
            class_names=class_names,
            filled=True,
            rounded=True,
            fontsize=FORMATTING.TREE_FONT_SIZE,
            proportion=True,  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
            impurity=False,  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º impurity
            precision=2,  # –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–∏—Å–µ–ª
        )

        plt.title(
            "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–∫—Ü–∏–π\n(–Ω–∞ –æ—Å–Ω–æ–≤–µ P/E, P/BV, ROE –∏ —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞ g)",
            fontsize=FORMATTING.TITLE_FONT_SIZE,
            fontweight="bold",
        )
        plt.tight_layout()
        plt.savefig(filename, dpi=FILE_CONSTANTS.DPI, bbox_inches="tight")
        plt.show()

        print(f"üìä –î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {filename}")

    def get_feature_importance(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –≤–∏–¥–µ DataFrame"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ train().")

        importance_df = pd.DataFrame(
            {
                "–ü—Ä–∏–∑–Ω–∞–∫": self.feature_columns,
                "–í–∞–∂–Ω–æ—Å—Ç—å": self.model.feature_importances_,
            }
        ).sort_values("–í–∞–∂–Ω–æ—Å—Ç—å", ascending=False)

        return importance_df

    def get_decision_rules(self, max_depth: int = None) -> list:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏–∑ –¥–µ—Ä–µ–≤–∞ –≤ —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        if self.model is None:
            return []

        from sklearn.tree import _tree

        tree = self.model.tree_
        feature_names = self.feature_columns
        class_names = [TARGET_MAPPING.LABELS[i] for i in sorted(self.model.classes_)]

        rules = []

        def recurse(node, depth, condition):
            if tree.feature[node] != _tree.TREE_UNDEFINED:  # –ù–µ –ª–∏—Å—Ç
                feature = feature_names[tree.feature[node]]
                threshold = tree.threshold[node]

                # –õ–µ–≤–∞—è –≤–µ—Ç–∫–∞ (<= threshold)
                left_condition = f"{feature} ‚â§ {threshold:.2f}"
                recurse(
                    tree.children_left[node], depth + 1, condition + [left_condition]
                )

                # –ü—Ä–∞–≤–∞—è –≤–µ—Ç–∫–∞ (> threshold)
                right_condition = f"{feature} > {threshold:.2f}"
                recurse(
                    tree.children_right[node], depth + 1, condition + [right_condition]
                )
            else:  # –õ–∏—Å—Ç
                if max_depth is None or depth <= max_depth:
                    samples = tree.n_node_samples[node]
                    if samples > 0:
                        class_dist = tree.value[node][0]
                        pred_class = np.argmax(class_dist)
                        confidence = class_dist[pred_class] / samples

                        if confidence > 0.5:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∞–≤–∏–ª–∞
                            rule = {
                                "conditions": " –∏ ".join(condition),
                                "prediction": class_names[pred_class],
                                "samples": samples,
                                "confidence": f"{confidence:.1%}",
                                "class_distribution": {
                                    class_names[i]: int(class_dist[i])
                                    for i in range(len(class_dist))
                                },
                            }
                            rules.append(rule)

        recurse(0, 0, [])
        return sorted(rules, key=lambda x: x["samples"], reverse=True)
